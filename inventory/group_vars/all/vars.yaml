# #############################################################################
# Standard variables.
# #############################################################################

# Cluster name.
cluster_name: 'goldentooth'

# The version of Kubernetes we're rolling with.
kubernetes_version: '1.32'

# This should always be 'python3'.
ansible_python_interpreter: 'python3'

# A clean, consistent version of the hostname.
clean_hostname: "{{ inventory_hostname | splitext | first | lower }}"

# Calculated IPv4 address for the current host.  Doesn't always work, usually does.
ipv4_address: "{{ ansible_default_ipv4.address | default(ansible_all_ipv4_addresses[0]) }}"

# #############################################################################
# Data structure for my personal preferences.
# #############################################################################

my:
  name:
    proper: 'Nathan'
    lower: 'nathan'
    full: 'Nathan Douglas'
  email:
    github: 'github@goldentooth.net'
  local:
    timezone: 'America/New_York'
  editor: 'nano'

# #############################################################################
# Data structure for the Ansible secret vault.
# #############################################################################

# secret_vault:
#   easy_password: <string>
#   ssh_public_key: <string>
#   github_token: <string>
#   aws:
#     access_key_id: <string>
#     secret_access_key: <string>
#   consul:
#     mgmt_token: <string>
#   step_ca:
#     passwords:
#       root: <string>
#       intermediate: <string>
#       provisioners:
#         jwk: <string>
#   vault:
#     recovery_keys:
#       - <string>
#     initial_root_token: <string>

# #############################################################################
# Shared variables.
# #############################################################################

# haproxy data:
haproxy:
  hostname: "{{ groups['haproxy'] | first }}"
  domain_name: 'lb-apiserver.kubernetes.local'
  ipv4_address: "{{ hostvars[groups['haproxy'] | first]['ipv4_address'] }}"
  nginx:
    user: 'www-data'
    group: 'www-data'
  # List of dicts defining additionally enabled collectors and their configuration.
  # It adds collectors to those enabled by default.
  #
  # Default:
  #   - 'systemd'
  #   - textfile:
  #       directory": "{{ node_exporter_textfile_dir }}"
  node_exporter:
    enabled_collectors:
      - 'systemd'

# Network data:
network:
  infrastructure:
    cidr: '10.4.0.0/20'
  service:
    cidr: '172.16.0.0/20'
  pod:
    cidr: '192.168.0.0/16'
  metallb:
    network: '10.4.11.0'
    cidr: '10.4.11.0/24'
    netmask: '255.255.255.0'
    interface: 'eth0'

# Cluster data:
cluster:
  name: "{{ cluster_name }}"
  domain: "{{ cluster_name }}.net"
  node_domain: "nodes.{{ cluster_name }}.net"
  services_domain: "services.{{ cluster_name }}.net"
  cloudfront_distribution_domain: "home-proxy.{{ cluster_name }}.net"
  cloudfront_origin_domain: "clearbrook.{{ cluster_name }}.net"
  ca_cert_path: "/etc/ssl/certs/{{ cluster_name }}.pem"
  github:
    organization: "{{ cluster_name }}"

# Host data:
# Here, "host" means that we're referring to a physical Raspberry Pi.
# This should reflect the existing network layout, which is configured
# within my router, etc. I'm working to evolve how this is configured,
# which is why there is some redundancy at present.
host:
  fqdn: "{{ clean_hostname }}.{{ cluster.domain }}"
  ip: "{{ ipv4_address }}"

# Node data:
# Here, "node" means that we're referring to a cluster entity.
# Why duplicate the "host" data above? I want to migrate the hosts to
# be <name>.nodes.goldentooth.net, to separate from a virtual services
# layer, etc, but that's a WIP.
node:
  fqdn: "{{ clean_hostname }}.{{ cluster.node_domain }}"
  ip: "{{ ipv4_address }}"

# Argo CD data:
argo_cd:
  # Argo CD chart version.
  chart_version: '7.1.5'
  # Argo CD chart repository URL.
  chart_repo_url: 'https://argoproj.github.io/argo-helm'
  # Argo CD release information.
  release_values:
    nameOverride: 'argocd'
    global:
      domain: "argocd.{{ cluster.domain }}"
      # Add Prometheus scrape annotations to all metrics services. This can
      # be used as an alternative to the ServiceMonitors.
      addPrometheusAnnotations: true
      # Default network policy rules used by all components
      networkPolicy:
        # -- Create NetworkPolicy objects for all components
        create: false
        # -- Default deny all ingress traffic
        defaultDenyIngress: false
    configs:
      secret:
        createSecret: true
        argocdServerAdminPassword: "{{ argocd_password }}"

    redis-ha:
      # Enable Redis high availability.
      enabled: true

    controller:
      replicas: 1

    server:
      autoscaling:
        enabled: true
        minReplicas: 2
      extraArgs:
        - '--insecure'
      service:
        annotations:
          external-dns.alpha.kubernetes.io/hostname: "argocd.{{ cluster.domain }}"
          external-dns.alpha.kubernetes.io/ttl: "60"
        type: 'LoadBalancer'

    repoServer:
      autoscaling:
        enabled: true
        minReplicas: 2

    applicationSet:
      replicas: 2
  namespace: 'argocd'

# Kubernetes data:
kubernetes:
  # The first control plane node.
  first: "{{ groups['k8s_control_plane'] | first }}"
  # The rest of the control plane nodes.
  rest: "{{ groups['k8s_control_plane'][1:] }}"
  # The Kubernetes config file path.
  admin_conf_path: '/etc/kubernetes/admin.conf'
  # The Calico project manifest file.
  calico_manifest_url: 'https://projectcalico.docs.tigera.io/manifests/calico.yaml'
  # The version of Kubernetes we're rolling with.
  version_clean: "{{ kubernetes_version }}"
  version: "v{{ kubernetes_version }}"
  # The Kubernetes certificate directory.
  pki_path: '/etc/kubernetes/pki'
  # The CRI socket URI.
  cri_socket_path: 'unix:///var/run/containerd/containerd.sock'
  # The Kubernetes packages to install from Apt.
  apt_packages:
    - 'kubeadm'
    - 'kubectl'
    - 'kubelet'
  # The Kubernetes Apt repository URL.
  apt_repo_url: "https://pkgs.k8s.io/core:/stable:/{{ kubernetes_version }}/deb/"

# NFS mount information.
nfs:
  server: "{{ groups['nfs_server'] | first}}"
  mounts:
    primary:
      share: "{{ hostvars[groups['nfs_server'] | first].ipv4_address }}:/mnt/usb1"
      mount: '/mnt/nfs'
      safe_name: 'mnt-nfs'
      type: 'nfs'
      options: {}

# Python data:
python:
  # The default Python version to use.
  default_version: '3.10'
  versions:
    - '3.10'
    - '3.11'
    - '3.12'
    - '3.13'

# Slurm options.
slurm_nfs_base_path: "{{ nfs.mounts.primary.mount }}/slurm"

# Slurm data:
slurm:
  lmod:
    source_url: "https://sourceforge.net/projects/lmod/files/Lmod-8.4.tar.bz2"
  nfs_base_path: "{{ slurm_nfs_base_path }}"
  # Python versions for Conda user environments.
  python_versions: "{{ python.versions }}"
  # Golang version.
  go_version: '1.23.0'
  shared_directories:
    - "{{ slurm_nfs_base_path }}"
    - "{{ slurm_nfs_base_path }}/var"
    - "{{ slurm_nfs_base_path }}/var/spool"
    - "{{ slurm_nfs_base_path }}/var/spool/slurm"
    - "{{ slurm_nfs_base_path }}/var/spool/slurm/ctld"
    - "{{ slurm_nfs_base_path }}/apps"
    - "{{ slurm_nfs_base_path }}/apps/modulefiles"
    - "{{ slurm_nfs_base_path }}/bin"
    - "{{ slurm_nfs_base_path }}/tmp"
  local_directories:
    - "/var/run/slurm"
    - "/var/spool/slurm"
    - "/var/spool/slurm/d"
  singularity_version: '4.3.0'

# (HashiCorp) Vault data:
vault:
  maybe_leader: "{{ groups['vault'] | first }}"
  aws:
    access_key_id: "{{ secret_vault.aws.access_key_id }}"
    secret_access_key: "{{ secret_vault.aws.secret_access_key }}"
    region: 'us-east-1'
  certs_path: '/opt/vault/tls'
  cert_path: '/opt/vault/tls/tls.crt'
  key_path: '/opt/vault/tls/tls.key'
  cluster_name: "{{ cluster.name }}"
  consul:
    policy_name: 'vault'
  data_path: '/opt/vault/data'
  etc_path: '/etc/vault.d'
  env_config_path: '/etc/vault.d/vault.env'
  hcl_config_path: '/etc/vault.d/vault.hcl'
  opt_path: '/opt/vault'
  raft_path: '/opt/vault/raft'
  seal_kms_key_alias: 'alias/goldentooth/vault-seal'

# Consul data:
consul:
  acl_bootstrap_reset_path: '/opt/consul/acl-bootstrap-reset'
  datacenter: 'dc1'
  domain: 'consul'
  etc_path: '/etc/consul.d'
  executable_path: '/usr/bin/consul'
  opt_path: '/opt/consul'
  maybe_leader: "{{ groups['consul_server'] | first }}"
  mgmt_token: "{{ secret_vault.consul.mgmt_token }}"
  role: "{{ 'server' if 'consul_server' in group_names else 'client' }}"
  certs_path: '/etc/consul.d/certs'
  cert_path: '/etc/consul.d/certs/tls.crt'
  key_path: '/etc/consul.d/certs/tls.key'
  hcl_config_path: '/etc/consul.d/consul.hcl'
  env_config_path: '/etc/consul.d/consul.env'
  agent_policy_name: "consul-agent-{{ clean_hostname }}"

# Nomad data:
nomad:
  maybe_leader: "{{ groups['nomad_server'] | first }}"
  role:  "{{ 'server' if 'nomad_server' in group_names else 'client' }}"
  service_user: "{{ 'nomad' if 'nomad_server' in group_names else 'root' }}"
  service_group: "{{ 'nomad' if 'nomad_server' in group_names else 'root' }}"
  datacenter: 'dc1'
  opt_path: '/opt/nomad'
  certs_path: '/opt/nomad/certs'
  cert_path: '/opt/nomad/certs/tls.crt'
  key_path: '/opt/nomad/certs/tls.key'
  cli_cert_path: '/opt/nomad/certs/cli.crt'
  cli_key_path: '/opt/nomad/certs/cli.key'
  etc_path: '/etc/nomad.d'
  data_path: '/opt/nomad/data'
  hcl_config_path: '/etc/nomad.d/nomad.hcl'
  env_config_path: '/etc/nomad.d/nomad.env'
  executable_path: '/usr/bin/nomad'
  is_consul_enabled: true

# Step-CA data:
step_ca:
  server: "{{ groups['step_ca'] | first }}"
  ca:
    bind_address: "{{ ipv4_address }}:9443"
    ca_config_file_path: '/etc/step-ca/config/ca.json'
    config_path: '/etc/step-ca/config'
    defaults_config_file_path: '/etc/step-ca/config/defaults.json'
    etc_path: '/etc/step-ca'
    executable: 'step-ca'
    password_file_paths:
      default_provisioner: '/etc/step-ca/.password_jwk_provisioner.txt'
      intermediate: "/etc/step-ca/.password_intermediate.txt"
      root: "/etc/step-ca/.password_root.txt"
    name: "{{ cluster.name }}"
    root_cert_path: '/etc/step-ca/certs/root_ca.crt'
    sans: "{{ host.fqdn }},{{ node.fqdn }},{{ ipv4_address }}"
    user: 'step'
  executable: 'step'
  root_cert_path: "/etc/ssl/certs/{{ cluster.name }}.pem"
  shared_cert_path: "/usr/local/share/ca-certificates/{{ cluster.name }}.crt"
  default_provisioner:
    name: 'default'
    password_path: '/root/.step/jwk_provisioner_password.txt'
    password:  "{{ secret_vault.step_ca.passwords.provisioners.jwk }}"

# Node Homepage data:
node_homepage:
  web_root: '/srv/status'
  nginx:
    user: 'www-data'
    group: 'www-data'

# Pi configuration data:
pi:
  # Fan
  #
  # Set the behaviour of a GPIO connected fan.
  #
  # `sudo raspi-config nonint do_fan <0/1> [gpio] [onTemp]`
  # - 0 - Enable fan
  # - 1 - Disable fan
  #
  # `gpio` defaults to 14.
  #
  # `onTemp` defaults to 80 Â°C.
  #
  fan_gpio: 14
  fan_temp: 60

  # Locale
  #
  # Select a locale, for example `en_US.UTF-8 UTF-8`.
  locale: 'en.US-UTF-8'

  # Time Zone
  #
  # Select a time zone, for example `America/New_York`.
  timezone: "{{ my.local.timezone }}"

  # Keyboard
  #
  # Select a keyboard layout, for example `us`.
  keyboard: 'us'

  # WLAN Country
  #
  # Select a WLAN country, for example `US`.
  wlan_country: 'US'

  # Overclock Frequency
  #
  # Set the overclocking frequency.
  overclock_freq: '2000'

  # Overclock Voltage
  #
  # Set the overclocking voltage.
  overclock_voltage: '6'

# Ray data.
ray:
  head:
    hostname: "{{ groups['ray_head'] | first }}"
    ipv4_address: "{{ hostvars[groups['ray_head'] | first]['ipv4_address'] }}"
  python_versions: "{{ python.versions }}"
  conda:
    environment_name: "python{{ python.default_version }}"
  role: "{{ 'head' if 'ray_head' in group_names else 'worker' }}"
  version: '2.46.0'

# Grafana data:
grafana:
  provisioners:
    dashboards:
      repository_name: 'grafana-dashboards'
  admin_password: "{{ secret_vault.grafana.admin_password }}"

# Loki data:
loki:
  consul:
    policy_name: 'loki'
  certs_path: '/etc/loki/tls'
  cert_path: '/etc/loki/tls/tls.crt'
  key_path: '/etc/loki/tls/tls.key'

# Vector data:
vector:
  consul:
    policy_name: 'vector'
  certs_path: '/etc/vector/tls'
  cert_path: '/etc/vector/tls/tls.crt'
  key_path: '/etc/vector/tls/tls.key'

# #############################################################################
# Variables for vendored roles.
# #############################################################################

# geerlingguy.containerd
# #############################################################################

containerd_package: 'containerd.io'
containerd_package_state: 'present'
containerd_service_state: 'started'
containerd_service_enabled: true
containerd_config_cgroup_driver_systemd: true

# geerlingguy.security
# #############################################################################

# Permit root login via SSH.
security_ssh_permit_root_login: 'yes'

# Allow only these users to log in via SSH.
security_ssh_allowed_users:
  - 'root'
  - "{{ my.name.lower }}"

# Allow the normal user account to run sudo without a password.
security_sudoers_passwordless:
  - "{{ my.name.lower }}"

# Enable automatic security updates.
security_autoupdate_enabled: true

# Reboot automatically after installing updates.
security_autoupdate_reboot: true

# Reboot at a different time, based on the host.
security_autoupdate_reboot_time: "0{{ host_index }}:00"

# Send an email to this address when updates are installed.
security_autoupdate_mail_to: "{{ clean_hostname }}@{{ cluster.domain }}"

# Send an email to this address if there's an error.
security_autoupdate_mail_on_error: true

# Don't worry about `fail2ban` for now.
security_fail2ban_enabled: true

# prometheus.prometheus.prometheus
# #############################################################################

# Prometheus package version. Also accepts `latest` as parameter.
prometheus_version: "3.2.1"

# The Agent mode optimizes Prometheus for the remote write use case. It
# disables querying, alerting, and local storage, and replaces it with a
# customized TSDB WAL. Everything else stays the same.
#
# This feature is available starting from Prometheus v2.32.0.
prometheus_agent_mode: false

# Alert relabeling rules. This should be specified as list in yaml format.
#
# It is compatible with the official alert_relabel_configs
prometheus_alert_relabel_configs: []

# Full list of alerting rules which will be copied to
# {{ prometheus_config_dir }}/rules/ansible_managed.yml.
#
# Alerting rules can be also provided by other files located in
# {{ prometheus_config_dir }}/rules/ which have a *.yml or *.yaml extension
#
# Please see default values in role defaults/main.yml
prometheus_alert_rules: []

# Configuration responsible for pointing where alertmanagers are. This should
# be specified as list in yaml format.
#
# It is compatible with the official alertmanager_config
prometheus_alertmanager_config: []

# Path to directory with prometheus configuration
prometheus_config_dir: '/etc/prometheus'

# Prometheus scrape jobs provided in same format as in the official docs
# See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config
prometheus_scrape_configs:
  - job_name: 'prometheus'
    metrics_path: '/metrics'
    static_configs:
      - targets:
          - "{{ clean_hostname }}:9090"
    relabel_configs:
      - source_labels: [instance]
        target_label: instance
        regex: '([^:]+):\d+'
        replacement: '${1}'
  - job_name: 'haproxy'
    static_configs:
      - targets:
          - "{{ groups['haproxy'] | first }}:8405"
    relabel_configs:
      - source_labels: [instance]
        target_label: instance
        regex: '([^:]+):\d+'
        replacement: '${1}'
  - job_name: 'nginx'
    static_configs:
      - targets:
          - "{{ groups['haproxy'] | first }}:9113"
    relabel_configs:
      - source_labels: [instance]
        target_label: instance
        regex: '([^:]+):\d+'
        replacement: '${1}'
  - job_name: 'unknown'
    file_sd_configs:
      - files:
          - "/etc/prometheus/file_sd/*.yaml"
          - "/etc/prometheus/file_sd/*.json"
    relabel_configs:
      - source_labels: [instance]
        target_label: instance
        regex: '([^:]+):\d+'
        replacement: '${1}'

# List of folders where ansible will look for files containing custom scrape
# config configuration files which will be copied to
# {{ prometheus_config_dir }}/scrape_configs/.
#
# This feature is available starting from Prometheus v2.43.0.
#
# Default:
#   - "prometheus/scrape_configs/*.yml"
#   - "prometheus/scrape_configs/*.json"
prometheus_scrape_config_files:
  - "{{ role_path }}/files/scrape_configs/*.yml"
  - "{{ role_path }}/files/scrape_configs/*.json"

# List of folders where ansible will look for files containing custom static
# target configuration files which will be copied to
# {{ prometheus_config_dir }}/file_sd/.
# Default:
#   - "prometheus/targets/*.yml"
#   - "prometheus/targets/*.json"
prometheus_static_targets_files:
  - "{{ role_path }}/files/static_targets/*.yml"
  - "{{ role_path }}/files/static_targets/*.json"

# Provide map of additional labels which will be added to any time series or
# alerts when communicating with external systems
#
# Default:
#   environment: "{{ ansible_fqdn | default(ansible_host) | default(inventory_hostname) }}"
prometheus_external_labels:
  environment: "{{ cluster.name }}"
  cluster: "{{ cluster.name }}"
  domain: "{{ cluster.domain }}"

# Prometheus global config. It is compatible with the official configuration
# See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#configuration-file
#
# Default:
#   evaluation_interval: "15s"
#   scrape_interval: "60s"
#   scrape_timeout: "15s"
prometheus_global:
  scrape_interval: "60s"
  evaluation_interval: "15s"
  scrape_timeout: "15s"

# Data retention period
prometheus_storage_retention: "15d"

# Data retention period by size
#
# Maximum number of bytes that can be stored for blocks.
#
# Units supported: KB, MB, GB, TB, PB.
prometheus_storage_retention_size: "5GB"

# Address on which prometheus will be listening
prometheus_web_listen_address: "0.0.0.0:9090"

# prometheus.prometheus.nginx_exporter
# #############################################################################
