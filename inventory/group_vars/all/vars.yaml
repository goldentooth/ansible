# #############################################################################
# Standard variables.
# #############################################################################

# This should always be 'python3'.
ansible_python_interpreter: 'python3'

# A clean, consistent version of the hostname.
clean_hostname: "{{ inventory_hostname | splitext | first | lower }}"

# Calculated IPv4 address for the current host.  Doesn't always work, usually does.
ipv4_address: "{{ ansible_default_ipv4.address | default(ansible_all_ipv4_addresses[0]) }}"

# #############################################################################
# Data structure for my personal preferences.
# #############################################################################

my:
  name:
    proper: 'Nathan'
    lower: 'nathan'
    full: 'Nathan Douglas'
  email:
    github: 'github@darkdell.net'
  local:
    timezone: 'America/New_York'
  domain:
    # Personal domain, e.g. blog, email, etc.
    personal: 'darkdell.net'
    # Private-facing domain, i.e. internal services.
    private: 'hellholt.net'
    # Public-facing domain, i.e. public services.
    public: 'bitterbridge.net'

# Data structure of the vault.
# vault:
#   easy_password: <string>
#   ssh_public_key: <string>
#   aws:
#     access_key_id: <string>
#     secret_access_key: <string>

# #############################################################################
# Shared variables.
# #############################################################################

# Load balancer data:
load_balancer:
  hostname: "{{ hostvars[groups['load_balancer'][0]]['clean_hostname'] }}"
  ipv4_address: "{{ hostvars[groups['load_balancer'][0]]['ipv4_address'] }}"

# Control plane data:
k8s_control_plane:
  all:
    hostnames: "{{ groups['k8s_control_plane'] | map('extract', hostvars, 'clean_hostname') | list }}"
    ipv4_addresses: "{{ groups['k8s_control_plane'] | map('extract', hostvars, 'ipv4_address') | list }}"
  first:
    hostname: "{{ hostvars[groups['k8s_control_plane'][0]]['clean_hostname'] }}"
    ipv4_address: "{{ hostvars[groups['k8s_control_plane'][0]]['ipv4_address'] }}"
  rest:
    hostnames: "{{ groups['k8s_control_plane'][1:] | map('extract', hostvars, 'clean_hostname') | list }}"
    ipv4_addresses: "{{ groups['k8s_control_plane'][1:] | map('extract', hostvars, 'ipv4_address') | list }}"

# k8s_worker data:
k8s_worker:
  hostnames: "{{ groups['k8s_worker'] | map('extract', hostvars, 'clean_hostname') | list }}"
  ipv4_addresses: "{{ groups['k8s_worker'] | map('extract', hostvars, 'ipv4_address') | list }}"

# Network data:
network:
  infrastructure:
    cidr: '10.4.0.0/20'
  service:
    cidr: '172.16.0.0/20'
  pod:
    cidr: '192.168.0.0/16'

# Cluster data:
cluster_name: 'goldentooth'
cluster:
  name: "{{ cluster_name }}"
  domain: "{{ cluster_name }}.net"
  cloudfront_distribution_domain: "home-proxy.{{ cluster_name }}.net"
  cloudfront_origin_domain: "clearbrook.{{ cluster_name }}.net"

# The version of Kubernetes we're rolling with.
k8s_version_clean: '1.32'
k8s_version: "v{{ k8s_version_clean }}"

# The Kubernetes packages to install from Apt.
k8s_packages:
  - 'kubeadm'
  - 'kubectl'
  - 'kubelet'

# NFS mount information.
nfs_server: "{{ groups['nfs_server'] }}"
nfs_mounts:
  Primary:
    share: "{{ hostvars[nfs_server[0]].ipv4_address }}:/mnt/usb1"
    mount: '/mnt/nfs'
    safe_name: 'mnt-nfs'
    type: 'nfs'
    options: {}

# Slurm options.
slurm:
  lmod:
    source_url: "https://sourceforge.net/projects/lmod/files/Lmod-8.4.tar.bz2"
  nfs_base_path: "{{ nfs_mounts['Primary']['mount'] }}/slurm"

# Golang version.
go_version: '1.23.0'

# Python versions for Conda user environments.
python_versions:
  - '3.10'
  - '3.11'
  - '3.12'
  - '3.13'

# #############################################################################
# Variables for vendored roles.
# #############################################################################

# geerlingguy.containerd
# #############################################################################

containerd_package: 'containerd.io'
containerd_package_state: 'present'
containerd_service_state: 'started'
containerd_service_enabled: true
containerd_config_cgroup_driver_systemd: true

# geerlingguy.security
# #############################################################################

# Permit root login via SSH.
security_ssh_permit_root_login: 'yes'

# Allow only these users to log in via SSH.
security_ssh_allowed_users:
  - 'root'
  - "{{ my.name.lower }}"

# Allow the normal user account to run sudo without a password.
security_sudoers_passwordless:
  - "{{ my.name.lower }}"

# Enable automatic security updates.
security_autoupdate_enabled: true

# Reboot automatically after installing updates.
security_autoupdate_reboot: true

# Reboot at a different time, based on the host.
security_autoupdate_reboot_time: "0{{ host_index }}:00"

# Send an email to this address when updates are installed.
security_autoupdate_mail_to: "{{ clean_hostname }}@{{ my.domain.private }}"

# Send an email to this address if there's an error.
security_autoupdate_mail_on_error: true

# Don't worry about `fail2ban` for now.
security_fail2ban_enabled: true

# prometheus.prometheus.prometheus
# #############################################################################

# Prometheus package version. Also accepts `latest` as parameter.
prometheus_version: "3.2.1"

# The Agent mode optimizes Prometheus for the remote write use case. It
# disables querying, alerting, and local storage, and replaces it with a
# customized TSDB WAL. Everything else stays the same.
#
# This feature is available starting from Prometheus v2.32.0.
prometheus_agent_mode: false

# Alert relabeling rules. This should be specified as list in yaml format.
#
# It is compatible with the official alert_relabel_configs
prometheus_alert_relabel_configs: []

# Full list of alerting rules which will be copied to
# {{ prometheus_config_dir }}/rules/ansible_managed.yml.
#
# Alerting rules can be also provided by other files located in
# {{ prometheus_config_dir }}/rules/ which have a *.yml or *.yaml extension
#
# Please see default values in role defaults/main.yml
prometheus_alert_rules: []

# Configuration responsible for pointing where alertmanagers are. This should
# be specified as list in yaml format.
#
# It is compatible with the official alertmanager_config
prometheus_alertmanager_config: []

# Path to directory with prometheus configuration
prometheus_config_dir: '/etc/prometheus'

# Prometheus scrape jobs provided in same format as in the official docs
# See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config
prometheus_scrape_configs:
  - job_name: 'prometheus'
    metrics_path: '/metrics'
    static_configs:
      - targets:
          - "{{ clean_hostname }}:9090"
  - job_name: 'haproxy'
    static_configs:
      - targets:
          - "{{ groups['load_balancer'] | first }}:8405"
  - job_name: 'nginx'
    static_configs:
      - targets:
          - "{{ groups['load_balancer'] | first }}:9113"
  - job_name: 'unknown'
    file_sd_configs:
      - files:
          - "/etc/prometheus/file_sd/*.yaml"
          - "/etc/prometheus/file_sd/*.json"

# List of folders where ansible will look for files containing custom scrape
# config configuration files which will be copied to
# {{ prometheus_config_dir }}/scrape_configs/.
#
# This feature is available starting from Prometheus v2.43.0.
#
# Default:
#   - "prometheus/scrape_configs/*.yml"
#   - "prometheus/scrape_configs/*.json"
prometheus_scrape_config_files:
  - "{{ role_path }}/files/scrape_configs/*.yml"
  - "{{ role_path }}/files/scrape_configs/*.json"

# List of folders where ansible will look for files containing custom static
# target configuration files which will be copied to
# {{ prometheus_config_dir }}/file_sd/.
# Default:
#   - "prometheus/targets/*.yml"
#   - "prometheus/targets/*.json"
prometheus_static_targets_files:
  - "{{ role_path }}/files/static_targets/*.yml"
  - "{{ role_path }}/files/static_targets/*.json"

# Provide map of additional labels which will be added to any time series or
# alerts when communicating with external systems
#
# Default:
#   environment: "{{ ansible_fqdn | default(ansible_host) | default(inventory_hostname) }}"
prometheus_external_labels:
  environment: "{{ cluster.name }}"
  cluster: "{{ cluster.name }}"
  domain: "{{ cluster.domain }}"

# Prometheus global config. It is compatible with the official configuration
# See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#configuration-file
#
# Default:
#   evaluation_interval: "15s"
#   scrape_interval: "60s"
#   scrape_timeout: "15s"
prometheus_global:
  scrape_interval: "60s"
  evaluation_interval: "15s"
  scrape_timeout: "15s"

# Data retention period
prometheus_storage_retention: "15d"

# Data retention period by size
#
# Maximum number of bytes that can be stored for blocks.
#
# Units supported: KB, MB, GB, TB, PB.
prometheus_storage_retention_size: "5GB"

# Address on which prometheus will be listening
prometheus_web_listen_address: "0.0.0.0:9090"

# prometheus.prometheus.nginx_exporter
# #############################################################################
