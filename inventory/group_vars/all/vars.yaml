# #############################################################################
# Standard variables.
# #############################################################################

# This should always be 'python3'.
ansible_python_interpreter: 'python3'

# A clean, consistent version of the hostname.
clean_hostname: "{{ inventory_hostname | splitext | first | lower }}"

# Calculated IPv4 address for the current host.  Doesn't always work, usually does.
ipv4_address: "{{ ansible_default_ipv4.address | default(ansible_all_ipv4_addresses[0]) }}"

# Cluster name.
cluster_name: 'goldentooth'

# #############################################################################
# Data structure for my personal preferences.
# #############################################################################

my:
  name:
    proper: 'Nathan'
    lower: 'nathan'
    full: 'Nathan Douglas'
  email:
    github: 'github@goldentooth.net'
  local:
    timezone: 'America/New_York'

# #############################################################################
# Data structure for the Ansible secret vault.
# #############################################################################

# secret_vault:
#   easy_password: <string>
#   ssh_public_key: <string>
#   github_token: <string>
#   aws:
#     access_key_id: <string>
#     secret_access_key: <string>
#   consul:
#     mgmt_token: <string>
#   step_ca:
#     passwords:
#       root: <string>
#       intermediate: <string>
#       provisioners:
#         jwk: <string>
#   vault:
#     recovery_keys:
#       - <string>
#     initial_root_token: <string>

# #############################################################################
# Shared variables.
# #############################################################################

# Load balancer data:
load_balancer:
  hostname: "{{ groups['load_balancer'] | first }}"
  ipv4_address: "{{ hostvars[groups['load_balancer'] | first]['ipv4_address'] }}"
  nginx:
    user: 'www-data'
    group: 'www-data'

# Control plane data:
k8s_control_plane:
  all:
    hostnames: "{{ groups['k8s_control_plane'] | map('extract', hostvars, 'clean_hostname') | list }}"
    ipv4_addresses: "{{ groups['k8s_control_plane'] | map('extract', hostvars, 'ipv4_address') | list }}"
  first:
    hostname: "{{ hostvars[groups['k8s_control_plane'][0]]['clean_hostname'] }}"
    ipv4_address: "{{ hostvars[groups['k8s_control_plane'][0]]['ipv4_address'] }}"
  rest:
    hostnames: "{{ groups['k8s_control_plane'][1:] | map('extract', hostvars, 'clean_hostname') | list }}"
    ipv4_addresses: "{{ groups['k8s_control_plane'][1:] | map('extract', hostvars, 'ipv4_address') | list }}"

# k8s_worker data:
k8s_worker:
  hostnames: "{{ groups['k8s_worker'] | map('extract', hostvars, 'clean_hostname') | list }}"
  ipv4_addresses: "{{ groups['k8s_worker'] | map('extract', hostvars, 'ipv4_address') | list }}"

# Network data:
network:
  infrastructure:
    cidr: '10.4.0.0/20'
  service:
    cidr: '172.16.0.0/20'
  pod:
    cidr: '192.168.0.0/16'

# Cluster data:
cluster:
  name: "{{ cluster_name }}"
  domain: "{{ cluster_name }}.net"
  node_domain: "nodes.{{ cluster_name }}.net"
  cloudfront_distribution_domain: "home-proxy.{{ cluster_name }}.net"
  cloudfront_origin_domain: "clearbrook.{{ cluster_name }}.net"
  ca_cert_path: "/etc/ssl/certs/{{ cluster_name }}.pem"

# Host data:
# Here, "host" means that we're referring to a physical Raspberry Pi.
# This should reflect the existing network layout, which is configured
# within my router, etc. I'm working to evolve how this is configured,
# which is why there is some redundancy at present.
host:
  fqdn: "{{ clean_hostname }}.{{ cluster.domain }}"
  ip: "{{ ipv4_address }}"

# Node data:
# Here, "node" means that we're referring to a cluster entity.
# Why duplicate the "host" data above? I want to migrate the hosts to
# be <name>.nodes.goldentooth.net, to separate from a virtual services
# layer, etc, but that's a WIP.
node:
  fqdn: "{{ clean_hostname }}.{{ cluster.node_domain }}"
  ip: "{{ ipv4_address }}"

# The version of Kubernetes we're rolling with.
k8s_version_clean: '1.32'
k8s_version: "v{{ k8s_version_clean }}"

# The Kubernetes Apt repository URL.
k8s_apt_repo_url: "https://pkgs.k8s.io/core:/stable:/{{ k8s_version }}/deb/"

# The Kubernetes packages to install from Apt.
k8s_packages:
  - 'kubeadm'
  - 'kubectl'
  - 'kubelet'

# NFS mount information.
nfs_server: "{{ groups['nfs_server'] }}"
nfs_mounts:
  Primary:
    share: "{{ hostvars[nfs_server[0]].ipv4_address }}:/mnt/usb1"
    mount: '/mnt/nfs'
    safe_name: 'mnt-nfs'
    type: 'nfs'
    options: {}

# Slurm options.
slurm_nfs_base_path: "{{ nfs_mounts['Primary']['mount'] }}/slurm"
slurm:
  lmod:
    source_url: "https://sourceforge.net/projects/lmod/files/Lmod-8.4.tar.bz2"
  nfs_base_path: "{{ slurm_nfs_base_path }}"
  # Python versions for Conda user environments.
  python_versions:
    - '3.10'
    - '3.11'
    - '3.12'
    - '3.13'
  # Golang version.
  go_version: '1.23.0'
  shared_directories:
    - "{{ slurm_nfs_base_path }}"
    - "{{ slurm_nfs_base_path }}/var"
    - "{{ slurm_nfs_base_path }}/var/spool"
    - "{{ slurm_nfs_base_path }}/var/spool/slurm"
    - "{{ slurm_nfs_base_path }}/var/spool/slurm/ctld"
    - "{{ slurm_nfs_base_path }}/apps"
    - "{{ slurm_nfs_base_path }}/apps/modulefiles"
    - "{{ slurm_nfs_base_path }}/bin"
    - "{{ slurm_nfs_base_path }}/tmp"
  local_directories:
    - "/var/run/slurm"
    - "/var/spool/slurm"
    - "/var/spool/slurm/d"
  singularity_version: '4.3.0'

# (HashiCorp) Vault data:
vault:
  maybe_leader: "{{ groups['vault'] | first }}"
  aws:
    access_key_id: "{{ secret_vault.aws.access_key_id }}"
    secret_access_key: "{{ secret_vault.aws.secret_access_key }}"
    region: 'us-east-1'
  certs_path: '/opt/vault/tls'
  cert_path: '/opt/vault/tls/tls.crt'
  key_path: '/opt/vault/tls/tls.key'
  cluster_name: "{{ cluster.name }}"
  consul:
    policy_name: 'vault'
  data_path: '/opt/vault/data'
  etc_path: '/etc/vault.d'
  env_config_path: '/etc/vault.d/vault.env'
  hcl_config_path: '/etc/vault.d/vault.hcl'
  opt_path: '/opt/vault'
  raft_path: '/opt/vault/raft'
  seal_kms_key_alias: 'alias/goldentooth/vault-seal'

# Consul data:
consul:
  acl_bootstrap_reset_path: '/opt/consul/acl-bootstrap-reset'
  datacenter: 'dc1'
  domain: 'consul'
  etc_path: '/etc/consul.d'
  executable_path: '/usr/bin/consul'
  opt_path: '/opt/consul'
  maybe_leader: "{{ groups['consul_server'] | first }}"
  mgmt_token: "{{ secret_vault.consul.mgmt_token }}"
  role: "{{ 'server' if 'consul_server' in group_names else 'client' }}"
  certs_path: '/etc/consul.d/certs'
  cert_path: '/etc/consul.d/certs/tls.crt'
  key_path: '/etc/consul.d/certs/tls.key'
  hcl_config_path: '/etc/consul.d/consul.hcl'
  env_config_path: '/etc/consul.d/consul.env'
  agent_policy_name: "consul-agent-{{ clean_hostname }}"

# Nomad data:
nomad:
  maybe_leader: "{{ groups['nomad_server'] | first }}"
  role:  "{{ 'server' if 'nomad_server' in group_names else 'client' }}"
  service_user: "{{ 'nomad' if 'nomad_server' in group_names else 'root' }}"
  service_group: "{{ 'nomad' if 'nomad_server' in group_names else 'root' }}"
  datacenter: 'dc1'
  opt_path: '/opt/nomad'
  certs_path: '/opt/nomad/certs'
  cert_path: '/opt/nomad/certs/tls.crt'
  key_path: '/opt/nomad/certs/tls.key'
  cli_cert_path: '/opt/nomad/certs/cli.crt'
  cli_key_path: '/opt/nomad/certs/cli.key'
  etc_path: '/etc/nomad.d'
  data_path: '/opt/nomad/data'
  hcl_config_path: '/etc/nomad.d/nomad.hcl'
  env_config_path: '/etc/nomad.d/nomad.env'
  executable_path: '/usr/bin/nomad'
  is_consul_enabled: true

# Step-CA data:
step_ca:
  server: "{{ groups['step_ca'] | first }}"
  ca:
    bind_address: "{{ ipv4_address }}:9443"
    ca_config_file_path: '/etc/step-ca/config/ca.json'
    config_path: '/etc/step-ca/config'
    defaults_config_file_path: '/etc/step-ca/config/defaults.json'
    etc_path: '/etc/step-ca'
    executable: 'step-ca'
    password_file_paths:
      default_provisioner: '/etc/step-ca/.password_jwk_provisioner.txt'
      intermediate: "/etc/step-ca/.password_intermediate.txt"
      root: "/etc/step-ca/.password_root.txt"
    name: "{{ cluster.name }}"
    root_cert_path: '/etc/step-ca/certs/root_ca.crt'
    sans: "{{ host.fqdn }},{{ node.fqdn }},{{ ipv4_address }}"
    user: 'step'
  executable: 'step'
  root_cert_path: "/etc/ssl/certs/{{ cluster.name }}.pem"
  shared_cert_path: "/usr/local/share/ca-certificates/{{ cluster.name }}.crt"
  default_provisioner:
    name: 'default'
    password_path: '/root/.step/jwk_default_provisioner.password.txt'
    password:  "{{ secret_vault.step_ca.passwords.provisioners.jwk }}"

# Node Homepage data:
node_homepage:
  web_root: '/srv/status'
  nginx:
    user: 'www-data'
    group: 'www-data'

# #############################################################################
# Variables for vendored roles.
# #############################################################################

# geerlingguy.containerd
# #############################################################################

containerd_package: 'containerd.io'
containerd_package_state: 'present'
containerd_service_state: 'started'
containerd_service_enabled: true
containerd_config_cgroup_driver_systemd: true

# geerlingguy.security
# #############################################################################

# Permit root login via SSH.
security_ssh_permit_root_login: 'yes'

# Allow only these users to log in via SSH.
security_ssh_allowed_users:
  - 'root'
  - "{{ my.name.lower }}"

# Allow the normal user account to run sudo without a password.
security_sudoers_passwordless:
  - "{{ my.name.lower }}"

# Enable automatic security updates.
security_autoupdate_enabled: true

# Reboot automatically after installing updates.
security_autoupdate_reboot: true

# Reboot at a different time, based on the host.
security_autoupdate_reboot_time: "0{{ host_index }}:00"

# Send an email to this address when updates are installed.
security_autoupdate_mail_to: "{{ clean_hostname }}@{{ cluster.domain }}"

# Send an email to this address if there's an error.
security_autoupdate_mail_on_error: true

# Don't worry about `fail2ban` for now.
security_fail2ban_enabled: true

# prometheus.prometheus.prometheus
# #############################################################################

# Prometheus package version. Also accepts `latest` as parameter.
prometheus_version: "3.2.1"

# The Agent mode optimizes Prometheus for the remote write use case. It
# disables querying, alerting, and local storage, and replaces it with a
# customized TSDB WAL. Everything else stays the same.
#
# This feature is available starting from Prometheus v2.32.0.
prometheus_agent_mode: false

# Alert relabeling rules. This should be specified as list in yaml format.
#
# It is compatible with the official alert_relabel_configs
prometheus_alert_relabel_configs: []

# Full list of alerting rules which will be copied to
# {{ prometheus_config_dir }}/rules/ansible_managed.yml.
#
# Alerting rules can be also provided by other files located in
# {{ prometheus_config_dir }}/rules/ which have a *.yml or *.yaml extension
#
# Please see default values in role defaults/main.yml
prometheus_alert_rules: []

# Configuration responsible for pointing where alertmanagers are. This should
# be specified as list in yaml format.
#
# It is compatible with the official alertmanager_config
prometheus_alertmanager_config: []

# Path to directory with prometheus configuration
prometheus_config_dir: '/etc/prometheus'

# Prometheus scrape jobs provided in same format as in the official docs
# See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config
prometheus_scrape_configs:
  - job_name: 'prometheus'
    metrics_path: '/metrics'
    static_configs:
      - targets:
          - "{{ clean_hostname }}:9090"
    relabel_configs:
      - source_labels: [instance]
        target_label: instance
        regex: '([^:]+):\d+'
        replacement: '${1}'
  - job_name: 'haproxy'
    static_configs:
      - targets:
          - "{{ groups['load_balancer'] | first }}:8405"
    relabel_configs:
      - source_labels: [instance]
        target_label: instance
        regex: '([^:]+):\d+'
        replacement: '${1}'
  - job_name: 'nginx'
    static_configs:
      - targets:
          - "{{ groups['load_balancer'] | first }}:9113"
    relabel_configs:
      - source_labels: [instance]
        target_label: instance
        regex: '([^:]+):\d+'
        replacement: '${1}'
  - job_name: 'unknown'
    file_sd_configs:
      - files:
          - "/etc/prometheus/file_sd/*.yaml"
          - "/etc/prometheus/file_sd/*.json"
    relabel_configs:
      - source_labels: [instance]
        target_label: instance
        regex: '([^:]+):\d+'
        replacement: '${1}'

# List of folders where ansible will look for files containing custom scrape
# config configuration files which will be copied to
# {{ prometheus_config_dir }}/scrape_configs/.
#
# This feature is available starting from Prometheus v2.43.0.
#
# Default:
#   - "prometheus/scrape_configs/*.yml"
#   - "prometheus/scrape_configs/*.json"
prometheus_scrape_config_files:
  - "{{ role_path }}/files/scrape_configs/*.yml"
  - "{{ role_path }}/files/scrape_configs/*.json"

# List of folders where ansible will look for files containing custom static
# target configuration files which will be copied to
# {{ prometheus_config_dir }}/file_sd/.
# Default:
#   - "prometheus/targets/*.yml"
#   - "prometheus/targets/*.json"
prometheus_static_targets_files:
  - "{{ role_path }}/files/static_targets/*.yml"
  - "{{ role_path }}/files/static_targets/*.json"

# Provide map of additional labels which will be added to any time series or
# alerts when communicating with external systems
#
# Default:
#   environment: "{{ ansible_fqdn | default(ansible_host) | default(inventory_hostname) }}"
prometheus_external_labels:
  environment: "{{ cluster.name }}"
  cluster: "{{ cluster.name }}"
  domain: "{{ cluster.domain }}"

# Prometheus global config. It is compatible with the official configuration
# See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#configuration-file
#
# Default:
#   evaluation_interval: "15s"
#   scrape_interval: "60s"
#   scrape_timeout: "15s"
prometheus_global:
  scrape_interval: "60s"
  evaluation_interval: "15s"
  scrape_timeout: "15s"

# Data retention period
prometheus_storage_retention: "15d"

# Data retention period by size
#
# Maximum number of bytes that can be stored for blocks.
#
# Units supported: KB, MB, GB, TB, PB.
prometheus_storage_retention_size: "5GB"

# Address on which prometheus will be listening
prometheus_web_listen_address: "0.0.0.0:9090"

# prometheus.prometheus.nginx_exporter
# #############################################################################
