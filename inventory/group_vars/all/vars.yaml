# #############################################################################
# Standard variables.
# #############################################################################

# Cluster name.
cluster_name: 'goldentooth'

# The version of Kubernetes we're rolling with.
kubernetes_version: '1.32'

# This should always be 'python3'.
ansible_python_interpreter: 'python3'

# A clean, consistent version of the hostname.
clean_hostname: "{{ inventory_hostname | splitext | first | lower }}"

# Calculated IPv4 address for the current host.  Doesn't always work, usually does.
ipv4_address: "{{ ansible_default_ipv4.address | default(ansible_all_ipv4_addresses[0]) }}"

# #############################################################################
# Data structure for my personal preferences.
# #############################################################################

my:
  name:
    proper: 'Nathan'
    lower: 'nathan'
    full: 'Nathan Douglas'
  email:
    github: 'github@goldentooth.net'
  local:
    timezone: 'America/New_York'
  editor: 'nano'

# #############################################################################
# Data structure for the Ansible secret vault.
# #############################################################################

# secret_vault:
#   easy_password: <string>
#   ssh_public_key: <string>
#   github_token: <string>
#   anthropic:
#     api_key: <string>
#   aws:
#     access_key_id: <string>
#     secret_access_key: <string>
#   consul:
#     gossip_key: <string>
#     mgmt_token: <string>
#   step_ca:
#     passwords:
#       root: <string>
#       intermediate: <string>
#       provisioners:
#         jwk: <string>
#   vault:
#     recovery_keys:
#       - <string>
#     initial_root_token: <string>

# #############################################################################
# Shared variables.
# #############################################################################

# haproxy data:
haproxy:
  hostname: "{{ groups['haproxy'] | first }}"
  domain_name: 'lb-apiserver.kubernetes.local'
  ipv4_address: "{{ hostvars[groups['haproxy'] | first]['ipv4_address'] }}"
  nginx:
    user: 'www-data'
    group: 'www-data'
  # List of dicts defining additionally enabled collectors and their configuration.
  # It adds collectors to those enabled by default.
  #
  # Default:
  #   - 'systemd'
  #   - textfile:
  #       directory": "{{ node_exporter_textfile_dir }}"
  node_exporter:
    enabled_collectors:
      - 'systemd'

# Network data:
network:
  infrastructure:
    cidr: '10.4.0.0/20'
  service:
    cidr: '172.16.0.0/20'
  pod:
    cidr: '192.168.0.0/16'
  metallb:
    network: '10.4.11.0'
    cidr: '10.4.11.0/24'
    netmask: '255.255.255.0'
    interface: ""

# Cluster data:
cluster:
  name: "{{ cluster_name }}"
  domain: "{{ cluster_name }}.net"
  node_domain: "nodes.{{ cluster_name }}.net"
  services_domain: "services.{{ cluster_name }}.net"
  k8s_services_domain: "services.k8s.{{ cluster_name }}.net"
  nomad_services_domain: "services.nomad.{{ cluster_name }}.net"
  cloudfront_distribution_domain: "home-proxy.{{ cluster_name }}.net"
  cloudfront_origin_domain: "clearbrook.{{ cluster_name }}.net"
  ca_cert_path: "/etc/ssl/certs/{{ cluster_name }}.pem"
  github:
    organization: "{{ cluster_name }}"

# Host data:
# Here, "host" means that we're referring to a physical machine.
# This should reflect the existing network layout, which is configured
# within my router, etc. I'm working to evolve how this is configured,
# which is why there is some redundancy at present.
host:
  fqdn: "{{ clean_hostname }}.{{ cluster.domain }}"
  ip: "{{ ipv4_address }}"
  architecture: "{{ 'arm64' if ansible_architecture == 'aarch64' else 'amd64' }}"
  networking_service: "{{ 'networking' if 'all_pis' in group_names else 'systemd-networkd' }}"

# Node data:
# Here, "node" means that we're referring to a cluster entity.
# Why duplicate the "host" data above? I want to migrate the hosts to
# be <name>.nodes.goldentooth.net, to separate from a virtual services
# layer, etc, but that's a WIP.
node:
  fqdn: "{{ clean_hostname }}.{{ cluster.node_domain }}"
  ip: "{{ ipv4_address }}"

# Argo CD data:
argo_cd:
  # Argo CD chart version.
  chart_version: '7.1.5'
  # Argo CD chart repository URL.
  chart_repo_url: 'https://argoproj.github.io/argo-helm'
  # Argo CD release information.
  release_values:
    nameOverride: 'argocd'
    global:
      domain: "argocd.{{ cluster.k8s_services_domain }}"
      # Add Prometheus scrape annotations to all metrics services. This can
      # be used as an alternative to the ServiceMonitors.
      addPrometheusAnnotations: true
      # Default network policy rules used by all components
      networkPolicy:
        # -- Create NetworkPolicy objects for all components
        create: false
        # -- Default deny all ingress traffic
        defaultDenyIngress: false
    configs:
      secret:
        createSecret: true
        argocdServerAdminPassword: "{{ argocd_password }}"

    redis-ha:
      # Enable Redis high availability.
      enabled: true

    controller:
      replicas: 1

    server:
      autoscaling:
        enabled: true
        minReplicas: 2
      extraArgs:
        - '--insecure'
      service:
        annotations:
          external-dns.alpha.kubernetes.io/hostname: "argocd.{{ cluster.k8s_services_domain }}"
          external-dns.alpha.kubernetes.io/ttl: "60"
        type: 'LoadBalancer'

    repoServer:
      autoscaling:
        enabled: true
        minReplicas: 2

    applicationSet:
      replicas: 2
  namespace: 'argocd'

# Kubernetes data:
kubernetes:
  # The first control plane node.
  first: "{{ groups['k8s_control_plane'] | first }}"
  # The rest of the control plane nodes.
  rest: "{{ groups['k8s_control_plane'][1:] }}"
  # The Kubernetes config file path.
  admin_conf_path: '/etc/kubernetes/admin.conf'
  # The Calico project manifest file.
  calico_manifest_url: 'https://projectcalico.docs.tigera.io/manifests/calico.yaml'
  # The version of Kubernetes we're rolling with.
  version_clean: "{{ kubernetes_version }}"
  version: "v{{ kubernetes_version }}"
  # The Kubernetes certificate directory.
  pki_path: '/etc/kubernetes/pki'
  # The CRI socket URI.
  cri_socket_path: 'unix:///var/run/containerd/containerd.sock'
  # The Kubernetes packages to install from Apt.
  apt_packages:
    - 'kubeadm'
    - 'kubectl'
    - 'kubelet'
  # The Kubernetes Apt repository URL.
  apt_repo_url: "https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version }}/deb/"

# NFS mount information.
nfs:
  server: "{{ groups['nfs_server'] | first}}"
  mounts:
    primary:
      share: "{{ hostvars[groups['nfs_server'] | first].ipv4_address }}:/mnt/usb1"
      mount: '/mnt/nfs'
      safe_name: 'mnt-nfs'
      type: 'nfs'
      options:
        - 'soft'           # Allow mount operations to fail and return error
        - 'timeo=30'       # 3 second timeout (30 deciseconds)
        - 'retrans=2'      # Only 2 retry attempts before giving up
        - 'intr'           # Allow keyboard interrupts to cancel operations
        - 'bg'             # Retry mounts in background if they fail
        - 'nofail'         # Don't treat mount failures as fatal
        - '_netdev'        # Requires network (systemd dependency)
    gpu_storage:
      share: "{{ hostvars['velaryon'].ipv4_address }}:/mnt/gpu-storage"
      mount: '/mnt/gpu-storage'
      safe_name: 'mnt-gpu\x2dstorage'
      type: 'nfs'
      options:
        - 'soft'           # Allow mount operations to fail and return error
        - 'timeo=30'       # 3 second timeout (30 deciseconds)
        - 'retrans=2'      # Only 2 retry attempts before giving up
        - 'intr'           # Allow keyboard interrupts to cancel operations
        - 'bg'             # Retry mounts in background if they fail
        - 'nofail'         # Don't treat mount failures as fatal
        - '_netdev'        # Requires network (systemd dependency)

# Python data:
python:
  # The default Python version to use.
  default_version: '3.10'
  versions:
    - '3.10'
    - '3.11'
    - '3.12'
    - '3.13'

# Slurm options.
slurm_nfs_base_path: "{{ nfs.mounts.primary.mount }}/slurm"

# Slurm data:
slurm:
  lmod:
    source_url: "https://sourceforge.net/projects/lmod/files/Lmod-8.4.tar.bz2"
  nfs_base_path: "{{ slurm_nfs_base_path }}"
  # Python versions for Conda user environments.
  python_versions: "{{ python.versions }}"
  # Golang version.
  go_version: '1.23.0'
  shared_directories:
    - "{{ slurm_nfs_base_path }}"
    - "{{ slurm_nfs_base_path }}/var"
    - "{{ slurm_nfs_base_path }}/var/spool"
    - "{{ slurm_nfs_base_path }}/var/spool/slurm"
    - "{{ slurm_nfs_base_path }}/var/spool/slurm/ctld"
    - "{{ slurm_nfs_base_path }}/apps"
    - "{{ slurm_nfs_base_path }}/apps/modulefiles"
    - "{{ slurm_nfs_base_path }}/bin"
    - "{{ slurm_nfs_base_path }}/tmp"
  local_directories:
    - "/var/run/slurm"
    - "/var/spool/slurm"
    - "/var/spool/slurm/d"
  singularity_version: '4.3.0'
  nextflow_version: '24.10.0'

# (HashiCorp) Vault data:
vault:
  maybe_leader: "{{ groups['vault'] | first }}"
  aws:
    access_key_id: "{{ secret_vault.aws.access_key_id }}"
    secret_access_key: "{{ secret_vault.aws.secret_access_key }}"
    region: 'us-east-1'
  certs_path: '/opt/vault/tls'
  cert_path: '/opt/vault/tls/tls.crt'
  key_path: '/opt/vault/tls/tls.key'
  cluster_name: "{{ cluster.name }}"
  consul:
    policy_name: 'vault'
  data_path: '/opt/vault/data'
  etc_path: '/etc/vault.d'
  env_config_path: '/etc/vault.d/vault.env'
  hcl_config_path: '/etc/vault.d/vault.hcl'
  opt_path: '/opt/vault'
  raft_path: '/opt/vault/raft'
  seal_kms_key_alias: 'alias/goldentooth/vault-seal'

# Consul data:
consul:
  acl_bootstrap_reset_path: '/opt/consul/acl-bootstrap-reset'
  datacenter: 'dc1'
  domain: 'consul'
  etc_path: '/etc/consul.d'
  executable_path: '/usr/bin/consul'
  opt_path: '/opt/consul'
  maybe_leader: "{{ groups['consul_server'] | first }}"
  mgmt_token: "{{ secret_vault.consul.mgmt_token }}"
  role: "{{ 'server' if 'consul_server' in group_names else 'client' }}"
  certs_path: '/etc/consul.d/certs'
  cert_path: '/etc/consul.d/certs/tls.crt'
  key_path: '/etc/consul.d/certs/tls.key'
  hcl_config_path: '/etc/consul.d/consul.hcl'
  env_config_path: '/etc/consul.d/consul.env'
  agent_policy_name: "consul-agent-{{ clean_hostname }}"

# Nomad data:
nomad:
  maybe_leader: "{{ groups['nomad_server'] | first }}"
  role:  "{{ 'server' if 'nomad_server' in group_names else 'client' }}"
  service_user: "{{ 'nomad' if 'nomad_server' in group_names else 'root' }}"
  service_group: "{{ 'nomad' if 'nomad_server' in group_names else 'root' }}"
  datacenter: 'dc1'
  opt_path: '/opt/nomad'
  certs_path: '/opt/nomad/certs'
  cert_path: '/opt/nomad/certs/tls.crt'
  key_path: '/opt/nomad/certs/tls.key'
  cli_cert_path: '/opt/nomad/certs/cli.crt'
  cli_key_path: '/opt/nomad/certs/cli.key'
  etc_path: '/etc/nomad.d'
  data_path: '/opt/nomad/data'
  hcl_config_path: '/etc/nomad.d/nomad.hcl'
  env_config_path: '/etc/nomad.d/nomad.env'
  executable_path: '/usr/bin/nomad'
  is_consul_enabled: true
  client:
    node_class: "{{ 'gpu' if 'nomad_client_gpu' in group_names else 'default' }}"

# Step-CA data:
# Step-CA certificate authority configuration
step_ca:
  # The server running step-ca (Certificate Authority)
  server: "{{ groups['step_ca'] | first }}"

  # CA-specific configuration (used when setting up/managing the CA itself)
  ca:
    bind_address: "{{ ipv4_address }}:9443"
    ca_config_file_path: '/etc/step-ca/config/ca.json'
    config_path: '/etc/step-ca/config'
    defaults_config_file_path: '/etc/step-ca/config/defaults.json'
    etc_path: '/etc/step-ca'
    executable: 'step-ca'
    # Password file paths on the CA server (used during CA initialization)
    password_file_paths:
      default_provisioner: '/etc/step-ca/.password_jwk_provisioner.txt'
      intermediate: "/etc/step-ca/.password_intermediate.txt"
      root: "/etc/step-ca/.password_root.txt"
    name: "{{ cluster.name }}"
    root_cert_path: '/etc/step-ca/certs/root_ca.crt'
    sans: "{{ clean_hostname }},{{ host.fqdn }},{{ node.fqdn }},{{ ipv4_address }}"
    user: 'step'

  # Step CLI configuration
  executable: 'step'

  # CA certificate paths on client nodes
  root_cert_path: "/etc/ssl/certs/{{ cluster.name }}.pem"
  shared_cert_path: "/usr/local/share/ca-certificates/{{ cluster.name }}.crt"

  # Default provisioner configuration for certificate issuance/renewal
  # Used by client nodes when requesting certificates from the CA
  default_provisioner:
    name: 'default'
    # Password file path on client nodes (different from CA server path)
    password_path: '/root/.step/jwk_provisioner_password.txt'
    password:  "{{ secret_vault.step_ca.passwords.provisioners.jwk }}"

# Node Homepage data:
node_homepage:
  web_root: '/srv/status'
  nginx:
    user: 'www-data'
    group: 'www-data'

# Pi configuration data:
pi:
  # Fan
  #
  # Set the behaviour of a GPIO connected fan.
  #
  # `sudo raspi-config nonint do_fan <0/1> [gpio] [onTemp]`
  # - 0 - Enable fan
  # - 1 - Disable fan
  #
  # `gpio` defaults to 14.
  #
  # `onTemp` defaults to 80 °C.
  #
  fan_gpio: 14
  fan_temp: 60

  # Locale
  #
  # Select a locale, for example `en_US.UTF-8 UTF-8`.
  locale: 'en.US-UTF-8'

  # Time Zone
  #
  # Select a time zone, for example `America/New_York`.
  timezone: "{{ my.local.timezone }}"

  # Keyboard
  #
  # Select a keyboard layout, for example `us`.
  keyboard: 'us'

  # WLAN Country
  #
  # Select a WLAN country, for example `US`.
  wlan_country: 'US'

  # Overclock Frequency
  #
  # Set the overclocking frequency.
  overclock_freq: '2000'

  # Overclock Voltage
  #
  # Set the overclocking voltage.
  overclock_voltage: '6'

# Ray data.
ray:
  head:
    hostname: "{{ groups['ray_head'] | first }}"
    ipv4_address: "{{ hostvars[groups['ray_head'] | first]['ipv4_address'] }}"
  python_versions: "{{ python.versions }}"
  conda:
    environment_name: "python{{ python.default_version }}"
  role: "{{ 'head' if 'ray_head' in group_names else 'worker' }}"
  version: '2.46.0'

# Grafana data:
grafana:
  provisioners:
    dashboards:
      repository_name: 'grafana-dashboards'
  admin_password: "{{ secret_vault.grafana.admin_password }}"
  certs_path: '/etc/grafana/tls'
  cert_path: '/etc/grafana/tls/tls.crt'
  key_path: '/etc/grafana/tls/tls.key'

# Loki data:
loki:
  consul:
    policy_name: 'loki'
  certs_path: '/etc/loki/tls'
  cert_path: '/etc/loki/tls/tls.crt'
  key_path: '/etc/loki/tls/tls.key'

# SeaweedFS data:
seaweedfs:
  mount_path: '/mnt/seaweedfs-ssd'
  device: '/dev/sda'
  filesystem_type: 'ext4'
  allocations:
    data: '350Gi'
    index: '50Gi'
    filer: '100Gi'
  user: 'seaweedfs'
  uid: 985
  gid: 985
  storage_class_name: 'seaweedfs-ssd-storage'
  reclaim_policy: 'Delete'
  volume_binding_mode: 'WaitForFirstConsumer'
  namespace: 'seaweedfs'

# Vector data:
vector:
  consul:
    policy_name: 'vector'
  certs_path: '/etc/vector/tls'
  cert_path: '/etc/vector/tls/tls.crt'
  key_path: '/etc/vector/tls/tls.key'

# ZFS data:
zfs:
  arc_max: 134217728  # 128MB ARC size for low-RAM Pis
  pool:
    name: 'rpool'
    device: '/dev/sda'
  datasets:
    - name: 'data'
      mountpoint: '/data'

# #############################################################################
# Variables for vendored roles.
# #############################################################################

# geerlingguy.containerd
# #############################################################################

containerd_package: 'containerd.io'
containerd_package_state: 'present'
containerd_service_state: 'started'
containerd_service_enabled: true
containerd_config_cgroup_driver_systemd: true

# geerlingguy.security
# #############################################################################

# Permit root login via SSH.
security_ssh_permit_root_login: 'yes'

# Allow only these users to log in via SSH.
security_ssh_allowed_users:
  - 'root'
  - "{{ my.name.lower }}"

# Allow the normal user account to run sudo without a password.
security_sudoers_passwordless:
  - "{{ my.name.lower }}"

# Enable automatic security updates.
security_autoupdate_enabled: true

# Reboot automatically after installing updates.
security_autoupdate_reboot: true

# Reboot at a different time, based on the host.
security_autoupdate_reboot_time: "0{{ host_index }}:00"

# Send an email to this address when updates are installed.
security_autoupdate_mail_to: "{{ clean_hostname }}@{{ cluster.domain }}"

# Send an email to this address if there's an error.
security_autoupdate_mail_on_error: true

# Don't worry about `fail2ban` for now.
security_fail2ban_enabled: true

# prometheus.prometheus.prometheus
# #############################################################################

# Prometheus package version. Also accepts `latest` as parameter.
prometheus_version: "3.2.1"

# Blackbox Exporter configuration
blackbox_exporter_version: "0.25.0"
blackbox_exporter_port: 9115
blackbox_exporter_log_level: "info"
blackbox_exporter_log_format: "logfmt"
blackbox_exporter_nginx_proxy: false

# Prometheus Slurm Exporter configuration (rivosinc fork)
prometheus_slurm_exporter:
  version: "1.6.10"
  port: 9092
  tls_port: 8443
  user: "slurm-exporter"
  group: "slurm-exporter"

# Blackbox Exporter target lists
blackbox_https_internal_targets:
  - "https://consul.{{ cluster.services_domain }}"
  - "https://vault.{{ cluster.services_domain }}"
  - "https://nomad.{{ cluster.services_domain }}"
  - "https://grafana.{{ cluster.services_domain }}"
  - "https://argocd.{{ cluster.k8s_services_domain }}"
  - "https://loki.{{ cluster.services_domain }}"

blackbox_http_targets:
  # Direct IP access for immediate testing
  - "http://10.4.0.10:9090"  # Prometheus on allyrion
  # Future domain-based targets (when DNS is configured)
  - "https://prometheus.{{ cluster.k8s_services_domain }}"
  - "https://allyrion.{{ cluster.node_domain }}"
  - "https://bettley.{{ cluster.node_domain }}"
  - "https://cargyll.{{ cluster.node_domain }}"
  - "https://dalt.{{ cluster.node_domain }}"
  - "https://erenford.{{ cluster.node_domain }}"
  - "https://fenn.{{ cluster.node_domain }}"
  - "https://gardener.{{ cluster.node_domain }}"
  - "https://harlton.{{ cluster.node_domain }}"
  - "https://inchfield.{{ cluster.node_domain }}"
  - "https://jast.{{ cluster.node_domain }}"
  - "https://karstark.{{ cluster.node_domain }}"
  - "https://lipps.{{ cluster.node_domain }}"
  - "https://velaryon.{{ cluster.node_domain }}"

blackbox_https_external_targets:
  - "https://clearbrook.{{ cluster.domain }}"
  - "https://home-proxy.{{ cluster.domain }}"
  - "https://clog.{{ cluster.domain }}"

blackbox_tcp_targets:
  # Direct IP access for immediate testing
  - "10.4.0.11:8500"  # Consul HTTP API (bettley)
  - "10.4.0.11:8200"  # Vault API (bettley)
  - "10.4.0.11:4646"  # Nomad API (bettley)
  - "10.4.0.19:9443"  # Step-CA (jast)
  - "10.4.0.10:6443"  # K8s API via HAProxy (allyrion)
  # Future domain-based targets (when DNS is configured)
  - "consul.{{ cluster.services_domain }}:443"
  - "vault.{{ cluster.services_domain }}:443"
  - "nomad.{{ cluster.services_domain }}:443"

blackbox_dns_targets:
  - "{{ cluster.domain }}"
  - "{{ cluster.node_domain }}"
  - "{{ cluster.services_domain }}"
  - "k8s.{{ cluster.domain }}"
  - "consul.{{ cluster.services_domain }}"
  - "vault.{{ cluster.services_domain }}"
  - "nomad.{{ cluster.services_domain }}"
  - "grafana.{{ cluster.services_domain }}"

blackbox_http_auth_targets:
  # Direct IP access for immediate testing
  - "http://10.4.0.10:8404/stats"  # HAProxy stats with admin auth
  # Future domain-based targets (when DNS is configured)
  - "https://haproxy.{{ cluster.services_domain }}/stats"

blackbox_api_targets:
  # Direct IP access for immediate testing
  - "http://10.4.0.11:8500/v1/status/leader"  # Consul leader (bettley)
  - "http://10.4.0.11:8200/v1/sys/health"     # Vault health (bettley)
  - "http://10.4.0.11:4646/v1/status/leader"  # Nomad leader (bettley)
  - "http://10.4.0.18:3100/ready"             # Loki ready (inchfield)
  # Future domain-based targets (when DNS is configured)
  - "https://consul.{{ cluster.services_domain }}/v1/status/leader"
  - "https://vault.{{ cluster.services_domain }}/v1/sys/health"
  - "https://nomad.{{ cluster.services_domain }}/v1/status/leader"
  - "https://loki.{{ cluster.services_domain }}/ready"

# Blackbox Exporter certificate paths
step_ca_cert_path: "{{ step_ca.root_cert_path }}"
step_ca_client_cert_path: "/etc/blackbox-exporter/tls/tls.crt"
step_ca_client_key_path: "/etc/blackbox-exporter/tls/tls.key"

# HAProxy stats authentication (for blackbox monitoring)
haproxy_stats_auth: "admin:{{ secret_vault.easy_password }}"

# The Agent mode optimizes Prometheus for the remote write use case. It
# disables querying, alerting, and local storage, and replaces it with a
# customized TSDB WAL. Everything else stays the same.
#
# This feature is available starting from Prometheus v2.32.0.
prometheus_agent_mode: false

# Alert relabeling rules. This should be specified as list in yaml format.
#
# It is compatible with the official alert_relabel_configs
prometheus_alert_relabel_configs: []

# Full list of alerting rules which will be copied to
# {{ prometheus_config_dir }}/rules/ansible_managed.yml.
#
# Alerting rules can be also provided by other files located in
# {{ prometheus_config_dir }}/rules/ which have a *.yml or *.yaml extension
#
# Please see default values in role defaults/main.yml
prometheus_alert_rules:
  # Basic Infrastructure Alerts
  - alert: NodeDown
    expr: up{job="node"} == 0
    for: 5m
    labels:
      severity: critical
      service: infrastructure
    annotations:
      summary: "Node is down"
      description: "Node has been down for more than 5 minutes"

  # Slurm Cluster Alerts
  - alert: SlurmHighPendingJobs
    expr: slurm_jobs_pending > 50
    for: 2m
    labels:
      severity: warning
      service: slurm
    annotations:
      summary: "High number of pending Slurm jobs"
      description: "High number of pending jobs in the Slurm queue"

  - alert: SlurmNodesDown
    expr: slurm_node_count_per_state{state="down"} > 0
    for: 1m
    labels:
      severity: critical
      service: slurm
    annotations:
      summary: "Slurm nodes are down"
      description: "Slurm compute nodes are currently down"

  - alert: SlurmHighCPUUtilization
    expr: (slurm_cpus_alloc / slurm_cpus_total) * 100 > 90
    for: 5m
    labels:
      severity: warning
      service: slurm
    annotations:
      summary: "High CPU utilization in Slurm cluster"
      description: "High CPU utilization across the Slurm cluster"

  - alert: SlurmHighMemoryUtilization
    expr: (slurm_mem_alloc / slurm_mem_real) * 100 > 95
    for: 5m
    labels:
      severity: critical
      service: slurm
    annotations:
      summary: "High memory utilization in Slurm cluster"
      description: "High memory utilization across the Slurm cluster"

  # Infrastructure Availability Alerts
  - alert: ServiceUnavailable
    expr: probe_success{job="blackbox"} == 0
    for: 2m
    labels:
      severity: critical
      service: infrastructure
    annotations:
      summary: "Service is unavailable"
      description: "Blackbox probe failed - service may be down"

  - alert: CertificateExpiration
    expr: goldentooth_certificate_expiry_days < 30
    for: 1m
    labels:
      severity: warning
      service: infrastructure
    annotations:
      summary: "Certificate expiring soon"
      description: "Certificate expiring soon"

  - alert: CertificateExpirationCritical
    expr: goldentooth_certificate_expiry_days < 7
    for: 1m
    labels:
      severity: critical
      service: infrastructure
    annotations:
      summary: "Certificate expiring very soon"
      description: "Certificate expiring soon"

  - alert: HighStorageUsage
    expr: (node_filesystem_size_bytes{fstype="ext4"} - node_filesystem_avail_bytes{fstype="ext4"}) / node_filesystem_size_bytes{fstype="ext4"} * 100 > 80
    for: 5m
    labels:
      severity: warning
      service: infrastructure
    annotations:
      summary: "High storage usage"
      description: "High storage usage detected"

  - alert: CriticalStorageUsage
    expr: (node_filesystem_size_bytes{fstype="ext4"} - node_filesystem_avail_bytes{fstype="ext4"}) / node_filesystem_size_bytes{fstype="ext4"} * 100 > 95
    for: 2m
    labels:
      severity: critical
      service: infrastructure
    annotations:
      summary: "Critical storage usage"
      description: "High storage usage detected"

# Configuration responsible for pointing where alertmanagers are. This should
# be specified as list in yaml format.
#
# It is compatible with the official alertmanager_config
prometheus_alertmanager_config: []

# Path to directory with prometheus configuration
prometheus_config_dir: '/etc/prometheus'

# Prometheus scrape jobs provided in same format as in the official docs
# See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config
prometheus_scrape_configs:
  - job_name: 'prometheus'
    metrics_path: '/metrics'
    static_configs:
      - targets:
          - "{{ clean_hostname }}:9090"
    relabel_configs:
      - source_labels: [instance]
        target_label: instance
        regex: '([^:]+):\d+'
        replacement: '${1}'
  - job_name: 'haproxy'
    static_configs:
      - targets:
          - "{{ groups['haproxy'] | first }}:8405"
    relabel_configs:
      - source_labels: [instance]
        target_label: instance
        regex: '([^:]+):\d+'
        replacement: '${1}'
  - job_name: 'nginx'
    static_configs:
      - targets:
          - "{{ groups['haproxy'] | first }}:9113"
    relabel_configs:
      - source_labels: [instance]
        target_label: instance
        regex: '([^:]+):\d+'
        replacement: '${1}'
  - job_name: 'blackbox-exporter'
    static_configs:
      - targets:
          - "{{ groups['prometheus'] | first }}:{{ blackbox_exporter_port }}"
    relabel_configs:
      - source_labels: [instance]
        target_label: instance
        regex: '([^:]+):\d+'
        replacement: '${1}'
  - job_name: 'node'
    file_sd_configs:
      - files:
          - "/etc/prometheus/file_sd/node.yaml"
    relabel_configs:
      - source_labels: [instance]
        target_label: instance
        regex: '([^:]+):\d+'
        replacement: '${1}'
  - job_name: 'loki'
    static_configs:
      - targets:
          - "{{ groups['loki'] | first }}:3100"
        labels:
          instance: "{{ groups['loki'] | first }}"
          job: 'loki'
    scheme: 'https'
    tls_config:
      ca_file: "{{ step_ca.root_cert_path }}"
  - job_name: 'nomad'
    metrics_path: '/v1/metrics'
    params:
      format: ['prometheus']
    static_configs:
      - targets:
          - "10.4.0.11:4646"  # bettley (server)
          - "10.4.0.12:4646"  # cargyll (server)
          - "10.4.0.13:4646"  # dalt (server)
          - "10.4.0.14:4646"  # erenford (client)
          - "10.4.0.15:4646"  # fenn (client)
          - "10.4.0.16:4646"  # gardener (client)
          - "10.4.0.17:4646"  # harlton (client)
          - "10.4.0.18:4646"  # inchfield (client)
          - "10.4.0.19:4646"  # jast (client)
          - "10.4.0.20:4646"  # karstark (client)
          - "10.4.0.21:4646"  # lipps (client)
          - "10.4.0.30:4646"  # velaryon (client)
    scheme: 'https'
    tls_config:
      ca_file: "{{ step_ca.root_cert_path }}"
      cert_file: "/etc/prometheus/certs/nomad-client.crt"
      key_file: "/etc/prometheus/certs/nomad-client.key"
    relabel_configs:
      - source_labels: [instance]
        target_label: instance
        regex: '([^:]+):\d+'
        replacement: '${1}'
  - job_name: 'consul'
    metrics_path: '/v1/agent/metrics'
    params:
      format: ['prometheus']
    static_configs:
      - targets:
          # Consul servers
          - "{{ groups['consul_server'][0] }}:8500"  # bettley (server)
          - "{{ groups['consul_server'][1] }}:8500"  # cargyll (server)
          - "{{ groups['consul_server'][2] }}:8500"  # dalt (server)
          # Consul clients
          - "allyrion:8500"     # allyrion (client)
          - "erenford:8500"     # erenford (client)
          - "fenn:8500"         # fenn (client)
          - "gardener:8500"     # gardener (client)
          - "harlton:8500"      # harlton (client)
          - "inchfield:8500"    # inchfield (client)
          - "jast:8500"         # jast (client)
          - "velaryon:8500"     # velaryon (client)
          - "karstark:8500"     # karstark (client)
          - "lipps:8500"        # lipps (client)
    scheme: 'http'
    relabel_configs:
      - source_labels: [instance]
        target_label: instance
        regex: '([^:]+):\d+'
        replacement: '${1}'
      - source_labels: [instance]
        target_label: consul_role
        regex: '(bettley|cargyll|dalt).*'
        replacement: 'server'
      - source_labels: [instance]
        target_label: consul_role
        regex: '(allyrion|erenford|fenn|gardener|harlton|inchfield|jast|velaryon|karstark|lipps).*'
        replacement: 'client'
  - job_name: 'vault'
    metrics_path: '/v1/sys/metrics'
    params:
      format: ['prometheus']
    static_configs:
      - targets:
          - "{{ groups['vault'][0] }}:8200"  # bettley (server)
          - "{{ groups['vault'][1] }}:8200"  # cargyll (server)
          - "{{ groups['vault'][2] }}:8200"  # dalt (server)
    scheme: 'https'
    tls_config:
      ca_file: "{{ step_ca.root_cert_path }}"
    relabel_configs:
      - source_labels: [instance]
        target_label: instance
        regex: '([^:]+):\d+'
        replacement: '${1}'
# Blackbox exporter jobs
  - job_name: 'blackbox'
    metrics_path: /probe
    file_sd_configs:
      - files:
          - "/etc/prometheus/file_sd/blackbox_targets.yaml"
    relabel_configs:
      # Use the target URL as the __param_target for blackbox exporter
      - source_labels: [__address__]
        target_label: __param_target
      # Set the actual target to the blackbox exporter
      - target_label: __address__
        replacement: '{{ groups["prometheus"] | first }}:{{ blackbox_exporter_port }}'
      # Preserve the original target as instance label
      - source_labels: [__param_target]
        target_label: instance
      # Map job labels to correct blackbox modules
      - source_labels: [job]
        target_label: __param_module
        regex: 'blackbox-http$'
        replacement: 'http_2xx'
      - source_labels: [job]
        target_label: __param_module
        regex: 'blackbox-https-internal'
        replacement: 'https_2xx_internal'
      - source_labels: [job]
        target_label: __param_module
        regex: 'blackbox-https-external'
        replacement: 'https_2xx_external'
      - source_labels: [job]
        target_label: __param_module
        regex: 'blackbox-tcp'
        replacement: 'tcp_connect'
      - source_labels: [job]
        target_label: __param_module
        regex: 'blackbox-dns'
        replacement: 'dns_query'
      - source_labels: [job]
        target_label: __param_module
        regex: 'blackbox-icmp'
        replacement: 'icmp_ping'
      - source_labels: [job]
        target_label: __param_module
        regex: 'blackbox-http-auth'
        replacement: 'http_2xx_auth'
      - source_labels: [job]
        target_label: __param_module
        regex: 'blackbox-api'
        replacement: 'https_api_2xx'
# Slurm exporter
  - job_name: 'slurm'
    file_sd_configs:
      - files:
          - "/etc/prometheus/file_sd/slurm_targets.yaml"
    relabel_configs:
      - source_labels: [instance]
        target_label: instance
        regex: '([^:]+):\d+'
        replacement: '${1}'
  # Vector internal metrics
  - job_name: 'vector'
    static_configs:
      - targets:
          - "10.4.0.10:9598"  # allyrion
          - "10.4.0.11:9598"  # bettley
          - "10.4.0.12:9598"  # cargyll
          - "10.4.0.13:9598"  # dalt
          - "10.4.0.14:9598"  # erenford
          - "10.4.0.15:9598"  # fenn
          - "10.4.0.16:9598"  # gardener
          - "10.4.0.17:9598"  # harlton
          - "10.4.0.18:9598"  # inchfield
          - "10.4.0.19:9598"  # jast
          - "10.4.0.20:9598"  # karstark
          - "10.4.0.21:9598"  # lipps
          - "10.4.0.30:9598"  # velaryon
    relabel_configs:
      - source_labels: [instance]
        target_label: instance
        regex: '([^:]+):\d+'
        replacement: '${1}'

# List of folders where ansible will look for files containing custom scrape
# config configuration files which will be copied to
# {{ prometheus_config_dir }}/scrape_configs/.
#
# This feature is available starting from Prometheus v2.43.0.
#
# Default:
#   - "prometheus/scrape_configs/*.yml"
#   - "prometheus/scrape_configs/*.json"
prometheus_scrape_config_files:
  - "{{ role_path }}/files/scrape_configs/*.yml"
  - "{{ role_path }}/files/scrape_configs/*.json"

# List of folders where ansible will look for files containing custom static
# target configuration files which will be copied to
# {{ prometheus_config_dir }}/file_sd/.
# Default:
#   - "prometheus/targets/*.yml"
#   - "prometheus/targets/*.json"
prometheus_static_targets_files:
  - "{{ role_path }}/files/static_targets/*.yml"
  - "{{ role_path }}/files/static_targets/*.json"

# Provide map of additional labels which will be added to any time series or
# alerts when communicating with external systems
#
# Default:
#   environment: "{{ ansible_fqdn | default(ansible_host) | default(inventory_hostname) }}"
prometheus_external_labels:
  environment: "{{ cluster.name }}"
  cluster: "{{ cluster.name }}"
  domain: "{{ cluster.domain }}"

# Prometheus global config. It is compatible with the official configuration
# See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#configuration-file
#
# Default:
#   evaluation_interval: "15s"
#   scrape_interval: "60s"
#   scrape_timeout: "15s"
prometheus_global:
  scrape_interval: "60s"
  evaluation_interval: "15s"
  scrape_timeout: "15s"

# Data retention period
prometheus_storage_retention: "15d"

# Data retention period by size
#
# Maximum number of bytes that can be stored for blocks.
#
# Units supported: KB, MB, GB, TB, PB.
prometheus_storage_retention_size: "5GB"

# Address on which prometheus will be listening
prometheus_web_listen_address: "0.0.0.0:9090"

# prometheus.prometheus.nginx_exporter
# #############################################################################
