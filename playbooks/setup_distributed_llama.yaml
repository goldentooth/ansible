# Description: Set up distributed-llama for Pi-only distributed LLM inference
# Playbook to set up distributed-llama across Raspberry Pi nodes only
# Cross-compilation on x86_64, deployment to Pi cluster only

- name: 'Set up distributed-llama with cross-compilation.'
  hosts: distributed_llama:velaryon
  become: false
  serial: "100%"  # Run all nodes in parallel after cross-compilation

  pre_tasks:
    - name: 'Display distributed-llama deployment plan.'
      ansible.builtin.debug:
        msg: |
          ğŸš€ Distributed LLaMA Deployment Plan:
          ğŸ“¦ Cross-compilation node: {{ distributed_llama.cross_compile.build_node }}
          ğŸ¯ Root node: {{ distributed_llama.root_node }}
          ğŸ‘¥ Worker nodes: {{ distributed_llama.worker_nodes | join(', ') }}
          ğŸ”§ Version: {{ distributed_llama.version }}
          ğŸ”¢ Total Pi nodes: {{ groups['distributed_llama'] | length }}
      run_once: true

    - name: 'Ensure prerequisites are met.'
      ansible.builtin.assert:
        that:
          - distributed_llama is defined
          - distributed_llama.cross_compile.enable | default(false)
          - distributed_llama.cross_compile.build_node is defined
        fail_msg: "Distributed-llama configuration is incomplete or invalid"
      run_once: true

  tasks:
    - name: 'Set up distributed-llama.'
      ansible.builtin.include_role:
        name: 'goldentooth.setup_distributed_llama'
      tags:
        - 'distributed_llama'
        - 'cross_compile'
        - 'build'
        - 'deploy'
        - 'user'
        - 'setup'

  post_tasks:
    - name: 'Display worker nodes status.'
      ansible.builtin.debug:
        msg: |
          âœ… Worker node {{ inventory_hostname }} configured
          ğŸ”§ Service: dllama-worker.service
          ğŸ“ Port: {{ distributed_llama.worker_port }}
          ğŸ§µ Threads: {{ distributed_llama.threads }}
      when: distributed_llama.enable_worker

    - name: 'Display root node status.'
      ansible.builtin.debug:
        msg: |
          âœ… Root node {{ inventory_hostname }} configured
          ğŸ”§ Service: dllama-root.service (manual start)
          ğŸ“ Models directory: {{ distributed_llama.models_dir }}
          ğŸŒ Will coordinate {{ distributed_llama.worker_nodes | length }} worker nodes
      when: distributed_llama.enable_root

    - name: 'Display final instructions.'
      ansible.builtin.debug:
        msg: |
          ğŸ‰ Distributed LLaMA setup complete!

          ğŸš€ Services running:
          âš¡ Worker nodes: Running and ready for distributed processing
          ğŸŒ API server: Running on {{ distributed_llama.root_node }}:{{ distributed_llama.api_port }}
          ğŸ§  Root service: Available for manual inference tasks

          ğŸ“‹ Usage:
          1. HTTP API requests:
             curl -X POST http://{{ distributed_llama.root_node }}:{{ distributed_llama.api_port }}/v1/chat/completions \
               -H "Content-Type: application/json" \
               -d '{"messages":[{"role":"user","content":"Hello!"}]}'

          2. Manual inference:
             goldentooth exec {{ distributed_llama.root_node }} "sudo systemctl start dllama-root"

          3. Monitor services:
             goldentooth exec all "systemctl status dllama-worker dllama-api"

          4. Check logs:
             goldentooth exec {{ distributed_llama.root_node }} "journalctl -u dllama-api -f"

          ğŸ“ Model files in: {{ distributed_llama.models_dir }}
      run_once: true

