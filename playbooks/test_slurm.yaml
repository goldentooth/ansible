---
- name: Slurm Integration Tests
  hosts: slurm_controller[0]  # Run tests from primary controller
  gather_facts: yes
  vars:
    test_results: []
    test_job_script: |
      #!/bin/bash
      #SBATCH --job-name=test_job
      #SBATCH --output=/tmp/slurm_test_%j.out
      #SBATCH --error=/tmp/slurm_test_%j.err
      #SBATCH --time=00:01:00
      #SBATCH --nodes=1
      #SBATCH --ntasks=1
      #SBATCH --partition=general
      
      echo "Slurm test job started at: $(date)"
      echo "Running on node: $(hostname)"
      echo "Job ID: $SLURM_JOB_ID"
      echo "Number of nodes: $SLURM_JOB_NUM_NODES"
      echo "Number of tasks: $SLURM_NTASKS"
      
      # Test basic commands
      echo "Testing basic system commands..."
      whoami
      uptime
      free -h
      
      # Test module system if available
      if command -v module &> /dev/null; then
          echo "Testing Lmod module system..."
          module list
          module avail
      fi
      
      echo "Slurm test job completed at: $(date)"

  tasks:
    - name: Create test directory
      file:
        path: /tmp/slurm_tests
        state: directory
        mode: '0755'

    - name: Test 1 - Check Slurm controller status
      shell: systemctl is-active slurmctld
      register: slurmctld_status
      ignore_errors: yes
      tags: [controller]

    - name: Test 2 - Check MUNGE authentication service
      shell: systemctl is-active munge
      register: munge_status
      ignore_errors: yes
      tags: [auth]

    - name: Test 3 - Verify Slurm cluster information
      shell: sinfo -h
      register: sinfo_result
      ignore_errors: yes
      tags: [cluster]

    - name: Test 4 - Check node status
      shell: sinfo -N -h
      register: node_status
      ignore_errors: yes
      tags: [nodes]

    - name: Test 5 - Test MUNGE credential encoding/decoding
      shell: echo "test" | munge | unmunge
      register: munge_test
      ignore_errors: yes
      tags: [auth]

    - name: Test 6 - Check available partitions
      shell: sinfo -s -h
      register: partitions
      ignore_errors: yes
      tags: [partitions]

    - name: Test 7 - Test srun with simple command
      shell: srun --partition=general --time=00:01:00 hostname
      register: srun_test
      ignore_errors: yes
      timeout: 90
      tags: [srun]

    - name: Test 8 - Create test batch script
      copy:
        content: "{{ test_job_script }}"
        dest: /tmp/slurm_tests/test_job.sh
        mode: '0755'
      tags: [sbatch]

    - name: Test 8a - Check test script permissions and content
      shell: |
        echo "=== Test script permissions ==="
        ls -la /tmp/slurm_tests/test_job.sh
        echo "=== Test script content ==="
        head -5 /tmp/slurm_tests/test_job.sh
        echo "=== Current user and directory ==="
        whoami
        pwd
        echo "=== /tmp directory permissions ==="
        ls -la /tmp/ | grep slurm_tests
      register: script_debug
      ignore_errors: yes
      tags: [sbatch]

    - name: Test 9 - Submit batch job with sbatch
      shell: sbatch /tmp/slurm_tests/test_job.sh
      register: sbatch_submit
      ignore_errors: yes
      tags: [sbatch]

    - name: Test 10 - Wait for job completion and check status
      shell: |
        if [[ "{{ sbatch_submit.stdout }}" =~ "Submitted batch job "([0-9]+) ]]; then
          job_id="${BASH_REMATCH[1]}"
          echo "Checking job $job_id"
          
          # Wait up to 60 seconds for job completion
          for i in {1..12}; do
            state=$(sacct -j $job_id --noheader --format=State | tr -d ' ')
            echo "Job state: $state"
            if [[ "$state" == "COMPLETED" || "$state" == "FAILED" || "$state" == "CANCELLED" ]]; then
              break
            fi
            sleep 5
          done
          
          # Get final job info
          sacct -j $job_id --format=JobID,JobName,State,ExitCode,Start,End,Elapsed
        else
          echo "Failed to extract job ID from: {{ sbatch_submit.stdout }}"
          exit 1
        fi
      register: job_status
      ignore_errors: yes
      when: sbatch_submit is succeeded
      tags: [sbatch, sacct]

    - name: Test 11 - Check Lmod module system
      shell: |
        if [ -f /mnt/nfs/slurm/lmod/lmod/init/bash ]; then
          echo "=== Lmod initialization file found ==="
          source /mnt/nfs/slurm/lmod/lmod/init/bash
          echo "=== Module system version ==="
          module --version
          echo "=== Available modules ==="
          module avail
          echo "=== Currently loaded modules ==="
          module list
          echo "=== Module search path ==="
          echo "MODULEPATH=$MODULEPATH"
        else
          echo "Lmod initialization file not found at /mnt/nfs/slurm/lmod/lmod/init/bash"
          exit 1
        fi
      register: lmod_test
      ignore_errors: yes
      tags: [lmod]

    - name: Test 12 - Test loading a module (if available)
      shell: |
        if [ -f /mnt/nfs/slurm/lmod/lmod/init/bash ]; then
          source /mnt/nfs/slurm/lmod/lmod/init/bash
          # Try to load OpenMPI module if available
          if module avail OpenMPI 2>&1 | grep -q OpenMPI; then
            echo "=== Loading OpenMPI module ==="
            module load OpenMPI
            echo "=== Loaded modules ==="
            module list
            echo "=== Checking for mpirun ==="
            which mpirun || echo "mpirun not found after loading OpenMPI"
          else
            echo "OpenMPI module not available"
            echo "Available modules:"
            module avail
          fi
          
          # Try to load Golang module  
          if module avail Golang 2>&1 | grep -q Golang; then
            echo "=== Loading Golang module ==="
            module load Golang/1.23.0
            echo "=== Checking for go ==="
            which go || echo "go not found after loading Golang"
            go version 2>/dev/null || echo "go version check failed"
          fi
        else
          echo "Lmod initialization file not available"
          exit 1
        fi
      register: module_load_test
      ignore_errors: yes
      tags: [lmod]

    - name: Test 13 - Check Slurm accounting
      shell: sacct --starttime=today --format=JobID,JobName,State,ExitCode --noheader | head -10
      register: accounting_test
      ignore_errors: yes
      tags: [accounting]

    - name: Test 14 - Check Slurm configuration
      shell: scontrol show config | grep -E "(ClusterName|ControlMachine|AccountingStorageType)"
      register: config_test
      ignore_errors: yes
      tags: [config]

    - name: Display test results summary
      debug:
        msg: |
          =================================================================
          SLURM INTEGRATION TEST RESULTS
          =================================================================
          
          Test 1 - Slurm Controller Status: {{ slurmctld_status.stdout | default('FAILED') }}
          Test 2 - MUNGE Status: {{ munge_status.stdout | default('FAILED') }}
          Test 3 - Cluster Info Available: {{ 'PASS' if sinfo_result is succeeded else 'FAIL' }}
          Test 4 - Node Status Check: {{ 'PASS' if node_status is succeeded else 'FAIL' }}
          Test 5 - MUNGE Auth Test: {{ 'PASS' if munge_test is succeeded and 'test' in munge_test.stdout else 'FAIL' }}
          Test 6 - Partitions Available: {{ 'PASS' if partitions is succeeded else 'FAIL' }}
          Test 7 - srun Command: {{ 'PASS' if srun_test is succeeded else 'FAIL' }}
          Test 8-10 - sbatch/sacct: {{ 'PASS' if sbatch_submit is succeeded and job_status is succeeded else 'FAIL' }}
          Test 11-12 - Lmod Modules: {{ 'PASS' if lmod_test is succeeded else 'FAIL' }}
          Test 13 - Accounting: {{ 'PASS' if accounting_test is succeeded else 'FAIL' }}
          Test 14 - Config Check: {{ 'PASS' if config_test is succeeded else 'FAIL' }}
          
          =================================================================
          DETAILED RESULTS
          =================================================================

    - name: Show cluster information
      debug:
        msg: |
          Cluster Information:
          {{ sinfo_result.stdout | default('N/A') }}
      when: sinfo_result is succeeded

    - name: Show node status
      debug:
        msg: |
          Node Status:
          {{ node_status.stdout | default('N/A') }}
      when: node_status is succeeded

    - name: Show partitions
      debug:
        msg: |
          Available Partitions:
          {{ partitions.stdout | default('N/A') }}
      when: partitions is succeeded

    - name: Show srun test result
      debug:
        msg: |
          srun Test Output:
          {{ srun_test.stdout | default('N/A') }}
      when: srun_test is succeeded

    - name: Show batch job results
      debug:
        msg: |
          Script Debug Info:
          {{ script_debug.stdout | default('N/A') }}
          
          Batch Job Submission:
          {{ sbatch_submit.stdout | default('N/A') }}
          
          {% if sbatch_submit.stderr %}
          Batch Job Error:
          {{ sbatch_submit.stderr }}
          {% endif %}
          
          Job Status:
          {{ job_status.stdout | default('N/A') }}
      when: sbatch_submit is defined

    - name: Show module system results
      debug:
        msg: |
          Module System Test:
          {{ lmod_test.stdout | default('N/A') }}
          
          Module Load Test:
          {{ module_load_test.stdout | default('N/A') }}
      when: lmod_test is succeeded

    - name: Show job output files (if available)
      shell: |
        echo "=== Recent Slurm job output files ==="
        ls -la /tmp/slurm_test_*.out 2>/dev/null | head -5 || echo "No output files found"
        echo ""
        echo "=== Latest job output content ==="
        latest_file=$(ls -t /tmp/slurm_test_*.out 2>/dev/null | head -1)
        if [ -n "$latest_file" ]; then
          echo "Contents of $latest_file:"
          cat "$latest_file"
        else
          echo "No job output files found"
        fi
      register: job_output
      ignore_errors: yes
      tags: [output]

    - name: Display job output
      debug:
        msg: "{{ job_output.stdout }}"
      when: job_output is succeeded

    - name: Cleanup test files
      file:
        path: "{{ item }}"
        state: absent
      loop:
        - /tmp/slurm_tests
        - /tmp/slurm_test_*.out
        - /tmp/slurm_test_*.err
      ignore_errors: yes
      tags: [cleanup]

    - name: Final test summary
      debug:
        msg: |
          =================================================================
          SLURM INTEGRATION TEST COMPLETE
          =================================================================
          
          Tests completed. Check the detailed results above for any failures.
          
          Quick Status:
          - Controller: {{ 'OK' if slurmctld_status is succeeded else 'FAIL' }}
          - MUNGE Auth: {{ 'OK' if munge_status is succeeded else 'FAIL' }}  
          - Job Submission: {{ 'OK' if srun_test is succeeded else 'FAIL' }}
          - Batch Jobs: {{ 'OK' if sbatch_submit is succeeded else 'FAIL' }}
          - Modules: {{ 'OK' if lmod_test is succeeded else 'FAIL' }}
          
          =================================================================