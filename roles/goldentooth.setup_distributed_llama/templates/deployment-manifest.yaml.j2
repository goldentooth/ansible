# Distributed LLaMA Deployment Manifest
# Generated by Ansible on {{ ansible_date_time.iso8601 }}

deployment:
  version: "{{ distributed_llama.version }}"
  build_date: "{{ ansible_date_time.iso8601 }}"
  build_node: "{{ distributed_llama.cross_compile.build_node }}"
  
  cluster:
    root_node: "{{ distributed_llama.root_node }}"
    worker_nodes: {{ distributed_llama.worker_nodes | to_yaml }}
    total_nodes: {{ distributed_llama.worker_nodes | length + 1 }}
    
  binaries:
    dllama: "{{ distributed_llama.binaries.dllama }}"
    dllama_api: "{{ distributed_llama.binaries.dllama_api }}"
    location: "{{ distributed_llama.install_dir }}/bin/"
    
  configuration:
    worker_port: {{ distributed_llama.worker_port }}
    api_port: {{ distributed_llama.api_port }}
    threads_per_node: "{{ distributed_llama.threads }}"
    buffer_float_type: "{{ distributed_llama.buffer_float_type }}"
    max_seq_len: {{ distributed_llama.max_seq_len }}
    models_dir: "{{ distributed_llama.models_dir }}"
    
  network:
    worker_addresses:
{% for host in distributed_llama.worker_nodes %}
{% if host != inventory_hostname %}
      - "{{ hostvars[host]['ipv4_address'] }}:{{ distributed_llama.worker_port }}"
{% endif %}
{% endfor %}

  usage:
    setup: "goldentooth setup_distributed_llama"
    build: "goldentooth build_distributed_llama" 
    status: "goldentooth dllama_status"
    start_workers: "goldentooth dllama_start_workers"
    run_inference: "goldentooth dllama_inference 'Hello world'"
    stop_all: "goldentooth dllama_stop"