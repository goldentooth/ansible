#!/bin/bash
set -euo pipefail

# Distributed LLaMA Root Node Startup Script
# Generated by Ansible - do not edit manually

echo "ðŸš€ Starting Distributed LLaMA Root Node on {{ inventory_hostname }}"

# Configuration
DLLAMA_BIN="{{ distributed_llama.install_dir }}/bin/dllama"
LOG_DIR="{{ distributed_llama.install_dir }}/logs"
MODELS_DIR="${DLLAMA_MODELS_DIR:-{{ distributed_llama.models_dir }}}"
THREADS="${DLLAMA_THREADS:-{{ distributed_llama.threads }}}"
BUFFER_FLOAT_TYPE="${DLLAMA_BUFFER_FLOAT_TYPE:-{{ distributed_llama.buffer_float_type }}}"
MAX_SEQ_LEN="${DLLAMA_MAX_SEQ_LEN:-{{ distributed_llama.max_seq_len }}}"

# Ensure directories exist
mkdir -p "${LOG_DIR}"
mkdir -p "${MODELS_DIR}"

# Log startup information
echo "$(date -Iseconds) - Starting root node on {{ inventory_hostname }}" >> "${LOG_DIR}/root.log"
echo "$(date -Iseconds) - Models directory: ${MODELS_DIR}" >> "${LOG_DIR}/root.log"

# Check if binary exists
if [ ! -x "${DLLAMA_BIN}" ]; then
    echo "âŒ Distributed LLaMA binary not found: ${DLLAMA_BIN}"
    exit 1
fi

# Discover worker nodes
echo "ðŸ” Discovering worker nodes..."
WORKER_ADDRESSES=""
{% for host in distributed_llama.worker_nodes %}
{% if host != inventory_hostname %}
WORKER_ADDRESSES="${WORKER_ADDRESSES}{{ hostvars[host]['ipv4_address'] }}:{{ distributed_llama.worker_port }},"
{% endif %}
{% endfor %}

# Remove trailing comma
WORKER_ADDRESSES="${WORKER_ADDRESSES%,}"

if [ -z "${WORKER_ADDRESSES}" ]; then
    echo "âš ï¸  No worker nodes discovered - running in single-node mode"
    WORKERS_ARG=""
else
    echo "âœ… Found worker nodes: ${WORKER_ADDRESSES}"
    WORKERS_ARG="--workers ${WORKER_ADDRESSES}"
fi

# Check for model files
MODEL_FILE=""
TOKENIZER_FILE=""

# Look for common model patterns
for model in "${MODELS_DIR}"/*.m; do
    if [ -f "${model}" ]; then
        MODEL_FILE="${model}"
        break
    fi
done

for tokenizer in "${MODELS_DIR}"/*.t; do
    if [ -f "${tokenizer}" ]; then
        TOKENIZER_FILE="${tokenizer}"
        break
    fi
done

if [ -z "${MODEL_FILE}" ] || [ -z "${TOKENIZER_FILE}" ]; then
    echo "âŒ Model or tokenizer files not found in ${MODELS_DIR}"
    echo "   Please place .m (model) and .t (tokenizer) files in the models directory"
    echo "   Example: dllama_model_meta-llama-3-8b_q40.m"
    echo "   Example: dllama_tokenizer_llama3.t"
    exit 1
fi

echo "ðŸ“ Using model: $(basename "${MODEL_FILE}")"
echo "ðŸ“ Using tokenizer: $(basename "${TOKENIZER_FILE}")"

# Start root node inference
echo "ðŸ”§ Starting root node with ${THREADS} threads"
exec "${DLLAMA_BIN}" inference \
    --model "${MODEL_FILE}" \
    --tokenizer "${TOKENIZER_FILE}" \
    --buffer-float-type "${BUFFER_FLOAT_TYPE}" \
    --max-seq-len "${MAX_SEQ_LEN}" \
    --nthreads "${THREADS}" \
    ${WORKERS_ARG} \
    "$@" \
    2>&1 | tee -a "${LOG_DIR}/root.log"